# -*- coding: utf-8 -*-
"""customer_chunk _prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10plI61DKNSNqkvUwPB-2lyiCfAJJL6Qb
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

data = pd.read_csv('/content/netflix_customer_churn.csv')

data

data.describe()

churn_counts = data['Churn'].value_counts()
display(churn_counts)

# Churn count distribution
sns.countplot(data=data, x='Churn')
plt.title("Churn Distribution")
plt.show()

data['Churn'].value_counts(normalize=True)

churn = data.loc[data["Churn"]==1]

Non_Churn = data.loc[data["Churn"]==0]

plt.figure(figsize=(12, 10))
sns.heatmap(data_encoded.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""# Task_1
Perform data cleaning and preprocessing

## Check for Missing Values
"""

missing_values = data.isnull().sum()
print("Missing values per column:\n", missing_values)

"""## Handle Duplicate Rows


"""

num_duplicates = data.duplicated().sum()
print(f"Number of duplicate rows found: {num_duplicates}")

print(f"Original shape of DataFrame: {data.shape}")
data_cleaned = data.drop_duplicates(keep='first')
num_duplicates_after_removal = data_cleaned.duplicated().sum()
print(f"Number of duplicate rows after removal: {num_duplicates_after_removal}")
print(f"New shape of DataFrame: {data_cleaned.shape}")
data = data_cleaned

"""## Encode Categorical Features


"""

data_encoded = pd.get_dummies(data, columns=['Gender', 'SubscriptionType'], drop_first=False)
print("First 5 rows of the DataFrame after one-hot encoding:")
print(data_encoded.head())
print("\nColumns after one-hot encoding:")
print(data_encoded.columns)

"""## Identify Numerical Columns

"""

numerical_columns_for_outlier_detection = ['Age', 'NumTransactions', 'Complaints']

print("Numerical columns selected for outlier detection:")
print(numerical_columns_for_outlier_detection)

outlier_summary = {}

for col_name in numerical_columns_for_outlier_detection:
    Q1 = data_encoded[col_name].quantile(0.25)
    Q3 = data_encoded[col_name].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers_col = data_encoded[(data_encoded[col_name] < lower_bound) | (data_encoded[col_name] > upper_bound)]

    outlier_summary[col_name] = {
        'Q1': Q1,
        'Q3': Q3,
        'IQR': IQR,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'num_outliers': len(outliers_col),
        'outliers_data': outliers_col.head()
    }

    print(f"\n--- Outlier Analysis for '{col_name}' ---")
    print(f"  Q1: {Q1}")
    print(f"  Q3: {Q3}")
    print(f"  IQR: {IQR}")
    print(f"  Lower Bound: {lower_bound}")
    print(f"  Upper Bound: {upper_bound}")
    print(f"  Number of outliers found: {len(outliers_col)}")

    if not outliers_col.empty:
        print("  First 5 outliers (if any):")
        display(outliers_col.head())
    else:
        print("  No outliers found in this column based on the IQR method.")

"""Feature_Engineering"""

if (data_encoded['NumTransactions'] == 0).any():
    print("Warning: 'NumTransactions' contains zero values. Replacing with 1 to avoid division by zero.")
    data_encoded['NumTransactions'] = data_encoded['NumTransactions'].replace(0, 1)

data_encoded['UsagePerTransaction'] = data_encoded['MonthlyUsageHours'] / data_encoded['NumTransactions']

# Replace any potential infinite values resulting from division by zero with 0
data_encoded['UsagePerTransaction'] = data_encoded['UsagePerTransaction'].replace([np.inf, -np.inf], 0)

print("First 5 rows with the new 'UsagePerTransaction' feature:")
print(data_encoded[['MonthlyUsageHours', 'NumTransactions', 'UsagePerTransaction']].head())

data_encoded['ComplaintRate'] = data_encoded['Complaints'] / data_encoded['NumTransactions']

# Replace any potential infinite values or NaN (if NumTransactions was 0 and Complaints was also 0) with 0
data_encoded['ComplaintRate'] = data_encoded['ComplaintRate'].replace([np.inf, -np.inf], 0).fillna(0)

print("First 5 rows with the new 'ComplaintRate' feature:")
print(data_encoded[['Complaints', 'NumTransactions', 'ComplaintRate']].head())

bins = [18, 29, 49, 70]
labels = ['Young', 'Middle-Aged', 'Senior']
data_encoded['AgeGroup'] = pd.cut(data_encoded['Age'], bins=bins, labels=labels, right=True, include_lowest=True)

print("First 5 rows with the new 'AgeGroup' feature:")
print(data_encoded[['Age', 'AgeGroup']].head())
print("\nValue counts for 'AgeGroup':")
print(data_encoded['AgeGroup'].value_counts())

"""## Display New Features

### Subtask:
Show the DataFrame with the newly engineered features to verify their creation.

"""

print("First 5 rows of data_encoded with new features:")
display(data_encoded[['Age', 'AgeGroup', 'MonthlyUsageHours', 'NumTransactions', 'UsagePerTransaction', 'Complaints', 'ComplaintRate']].head())



plt.figure(figsize=(8, 6))
sns.boxplot(data=data_encoded, x='Churn', y='UsagePerTransaction')
plt.title('UsagePerTransaction Distribution by Churn Status')
plt.xlabel('Churn Status')
plt.ylabel('Usage Per Transaction (Hours/Transaction)')
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(data=data_encoded, x='Churn', y='ComplaintRate')
plt.title('Complaint Rate Distribution by Churn Status')
plt.xlabel('Churn Status')
plt.ylabel('Complaint Rate')
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(data=data_encoded, x='AgeGroup', hue='Churn')
plt.title('Churn Distribution by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.show()

"""## Feature Scaling using StandardScaler"""

from sklearn.preprocessing import StandardScaler

# Identify numerical columns for scaling
# Exclude 'customer_id', 'Churn', and one-hot encoded 'Gender'/'SubscriptionType'/'AgeGroup' columns
numerical_cols_to_scale = [
    'Age',
    'MonthlyUsageHours',
    'NumTransactions',
    'Complaints',
    'UsagePerTransaction',
    'ComplaintRate'
]

# Initialize StandardScaler
scaler = StandardScaler()

# Create a copy of the DataFrame to store scaled data to avoid modifying original 'data_encoded' directly
data_scaled = data_encoded.copy()

# Apply StandardScaler to the selected numerical columns
data_scaled[numerical_cols_to_scale] = scaler.fit_transform(data_scaled[numerical_cols_to_scale])

print("First 5 rows of the DataFrame after Standard Scaling for numerical features:")
display(data_scaled[numerical_cols_to_scale].head())

print("\nDescriptive statistics of scaled numerical features:")
display(data_scaled[numerical_cols_to_scale].describe())

data_scaled = pd.get_dummies(data_scaled, columns=['AgeGroup'], drop_first=False)

X = data_scaled.drop(columns=['customer_id', 'Churn'])
y = data_scaled['Churn']

print("Shape of X (features):", X.shape)
print("Shape of y (target):", y.shape)

print("First 5 rows of X:")
display(X.head())
print("First 5 rows of y:")
display(y.head())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

from sklearn.linear_model import LogisticRegression


model = LogisticRegression(random_state=42, solver='liblinear')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Logistic Regression model trained successfully and predictions made.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix


accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt


y_proba = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc_score = roc_auc_score(y_test, y_proba)
print("AUC Score:", auc_score)

# Plot ROC curve
plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {auc_score:.3f})")
plt.plot([0,1], [0,1], linestyle='--', label="Random Classifier")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Initialize a Random Forest Classifier model
rf_model = RandomForestClassifier(random_state=42)

print("Random Forest Classifier model initialized successfully.")

rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)

print("Random Forest Classifier model trained and predictions made.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Calculate accuracy for Random Forest
accuracy_rf = accuracy_score(y_test, y_pred_rf)

# Calculate precision for Random Forest
precision_rf = precision_score(y_test, y_pred_rf)

# Calculate recall for Random Forest
recall_rf = recall_score(y_test, y_pred_rf)

# Calculate F1-score for Random Forest
f1_rf = f1_score(y_test, y_pred_rf)

# Generate confusion matrix for Random Forest
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)

print(f"--- Random Forest Model Evaluation ---")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-Score: {f1_rf:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_rf)

print("\n--- Comparison with Logistic Regression (Previous Model) ---")
print(f"Logistic Regression Accuracy: {accuracy:.4f}")
print(f"Random Forest Accuracy:     {accuracy_rf:.4f}\n")
print(f"Logistic Regression Precision: {precision:.4f}")
print(f"Random Forest Precision:     {precision_rf:.4f}\n")
print(f"Logistic Regression Recall: {recall:.4f}")
print(f"Random Forest Recall:     {recall_rf:.4f}\n")
print(f"Logistic Regression F1-Score: {f1:.4f}")
print(f"Random Forest F1-Score:     {f1_rf:.4f}")

